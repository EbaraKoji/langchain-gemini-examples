{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "llm = VertexAI(model_name='gemini-pro', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.embeddings.voyageai import VoyageEmbeddings\n",
    "from langchain.llms.huggingface_hub import HuggingFaceHub\n",
    "from langchain.embeddings.huggingface_hub import HuggingFaceHubEmbeddings\n",
    "\n",
    "\n",
    "embedding = VertexAIEmbeddings('textembedding-gecko@001')\n",
    "\n",
    "# XXX: 性能がgeckoと比較して著しく悪い!!!\n",
    "# embedding = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=os.environ['GOOGLE_GENERATIVE_LANGUAGE_API_KEY'])\n",
    "\n",
    "# XXX: ERROR!!!embed_contentメソッドが実装されていないのでretrieverのembeddingとして使えない!!\n",
    "# embedding = GoogleGenerativeAIEmbeddings(model='models/embedding-gecko-001', google_api_key=os.environ['GOOGLE_GENERATIVE_LANGUAGE_API_KEY'])\n",
    "\n",
    "# XXX: responseがとても遅い!性能も悪い！!\n",
    "# embedding = HuggingFaceHubEmbeddings(repo_id='intfloat/multilingual-e5-base')\n",
    "\n",
    "# TODO: API KEYの有効化(課金)、性能比較\n",
    "# embedding = VoyageEmbeddings(voyage_api_key=os.environ['CLAUDE_API_SECRET_KEY'])Ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.web_base import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(web_path='https://python.langchain.com/docs/modules/memory/')\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "retriever = Chroma.from_documents(documents=splits, embedding=embedding) \\\n",
    "    .as_retriever(search_kwargs={'k': min(len(docs), 4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever('How to add memory to a chain? Please explain with an code example.')\n",
    "result = retriever.get_relevant_documents('How to add memory to a chain? Please explain with an code example.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain provides a lot of utilities for adding memory to a system.\n",
      "These utilities can be used by themselves or incorporated seamlessly into a chain.Most of memory-related functionality in LangChain is marked as beta. This is for two reasons:Most functionality (with some exceptions, see below) are not production readyMost functionality (with some exceptions, see below) work with Legacy chains, not the newer LCEL syntax.The main exception to this is the ChatMessageHistory functionality. This functionality is largely production ready and does integrate with LCEL.LCEL Runnables: For an overview of how to use ChatMessageHistory with LCEL runnables, see these docsIntegrations: For an introduction to the various ChatMessageHistory integrations, see these docsIntroduction​A memory system needs to support two basic actions: reading and writing.\n",
      "Recall that every chain defines some core execution logic that expects certain inputs.\n"
     ]
    }
   ],
   "source": [
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation.',\n",
       " 'language': 'en',\n",
       " 'source': 'https://python.langchain.com/docs/modules/memory/',\n",
       " 'title': '[Beta] Memory | 🦜️🔗 Langchain'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata['source'] == result[0].metadata['source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "性能はClaude以外では圧倒的にgeckoモデルが強い。\n",
    "\n",
    "ただし、geckoモデルを使用しても、example codeは取得できていない。\n",
    "\n",
    "結局、geminiを使う場合はある程度長いcontextでも問題なく入れられるので、「基本はWebサイト全体のpage contentをcontextにそのまま代入」「複数のWebサイトをRAGに使う場合のみretrieverを使用してmetadataから該当するWebサイトを特定してやはりサイト全体のpage contentを代入」の方が精度がいい可能性が高い。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
