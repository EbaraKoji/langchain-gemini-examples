{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL\n",
    "\n",
    "[LCEL](https://python.langchain.com/docs/expression_language/) in official docs \\\n",
    "[tutorial video](https://www.youtube.com/watch?v=9M8x485j_lU) on Youtube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model='gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('tell me a joke about {foo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the bear join the circus?\\n\\nBecause he wanted to be a bare-back rider!')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'foo': 'bears'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the bear get kicked out of the restaurant?\\n\\nBecause he ate all the honey and left no tips!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({'foo': 'bears'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the bear get lost in the woods?')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm.bind(stop=['\\n'])\n",
    "chain.invoke({'foo': 'bears'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [{\n",
    "    'name': 'joke',\n",
    "    'description': 'A joke',\n",
    "    'parameters': {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'setup': {\n",
    "                'type': 'string',\n",
    "                'description': 'The setup for the joke'\n",
    "            },\n",
    "            'punchline': {\n",
    "                'type': 'string',\n",
    "                'description': 'The punchline for the joke'\n",
    "            }\n",
    "        },\n",
    "        'required': ['setup', 'punchline']\n",
    "    }\n",
    "}]\n",
    "\n",
    "chain = prompt | llm.bind(function_call={'name': 'joke'}, functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/5c_kkmr56nl0nv3f4nty4mym0000gn/T/ipykernel_99440/4238034826.py\", line 5, in <module>\n",
      "    chain.invoke({'foo', 'bears'}, config={})\n",
      "  File \"/Users/ebarakoji/miniforge3/envs/langchain_gemini/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2053, in invoke\n",
      "    input = step.invoke(\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/Users/ebarakoji/miniforge3/envs/langchain_gemini/lib/python3.11/site-packages/langchain_core/prompts/base.py\", line 112, in invoke\n",
      "    return self._call_with_config(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ebarakoji/miniforge3/envs/langchain_gemini/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1246, in _call_with_config\n",
      "    context.run(\n",
      "  File \"/Users/ebarakoji/miniforge3/envs/langchain_gemini/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 326, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ebarakoji/miniforge3/envs/langchain_gemini/lib/python3.11/site-packages/langchain_core/prompts/base.py\", line 91, in _format_prompt_with_error_handling\n",
      "    raise TypeError(\n",
      "TypeError: Expected mapping type as input to ChatPromptTemplate. Received <class 'set'>.\n"
     ]
    }
   ],
   "source": [
    "# openai-functionはgeminiでは使えない\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    chain.invoke({'foo', 'bears'}, config={})\n",
    "except TypeError:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description='question to set up a joke')\n",
    "    punchline: str = Field(description='answer to resolve the joke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'What do you call a bear with no teeth?',\n",
       " 'punchline': 'A gummy bear!'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='Tell me a joke about {name}.\\n{format_instructions}\\n',\n",
    "    input_variables=['name'],\n",
    "    partial_variables={'format_instructions': parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({'name': 'bear'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paththroughs and itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_google_vertexai.embeddings import VertexAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorestore = Chroma.from_texts(['harrison worked at kensho'], embedding=VertexAIEmbeddings('textembedding-gecko@001'))\n",
    "retriever = vectorestore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ({'context': retriever, 'question': RunnablePassthrough()} | prompt | llm | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kensho'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('where did harrison work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "template = '''Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\n",
    "    'context': itemgetter('question') | retriever,\n",
    "    'question': itemgetter('question'),\n",
    "    'language': itemgetter('language')\n",
    "} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' harrisonはkenshoで働いていました'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question': 'where did harrison work', 'language': 'japanese'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple LLM Chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs: [manipulating inputs & output](https://python.langchain.com/docs/expression_language/how_to/map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ホノルルはアメリカ合衆国ハワイ州にあります。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template('what is the city {person} is from?')\n",
    "prompt2 = ChatPromptTemplate.from_template('what country is the city {city} in? respond in {language}.')\n",
    "\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "chain2 = ({'city': chain1, 'language': itemgetter('language')} | prompt2 | llm | StrOutputParser())\n",
    "chain2.invoke({'person': 'obama', 'language': 'japanese'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template('generate a random color name.')\n",
    "prompt2 = ChatPromptTemplate.from_template('what is a fruit of color: {color}')\n",
    "prompt3 = ChatPromptTemplate.from_template('what is the coutries flag that has the color: {color}')\n",
    "prompt4 = ChatPromptTemplate.from_template('what is the color of {fruit} and {coutry}')\n",
    "\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "chain2 = (RunnableParallel(color=chain1)\n",
    "          | {\n",
    "              'fruit': prompt2 | llm | StrOutputParser(),\n",
    "              'coutry': prompt3 | llm | StrOutputParser(),\n",
    "}\n",
    "    | prompt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='what is the color of Kiwi and Libya')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.ddg_search import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''turn the following user input into a search query for a search engine: \n",
    "{input}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser() | search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TIME ET. TV. Indiana at Boston. 7:30pm. TNT. The Boston Celtics leads all time series 125-97 versus Indiana Pacers. Last season the Celtics lead regular season series 2-1 versus the Pacers. Boston is 67-28 all time at home versus Indiana. The Celtics and Pacers 2023-24 regular season series is tied 2-2. Sports Games Today has established itself as a leading and trusted online platform for up-to-date live sports TV schedules, ensuring that fans are well-informed and engaged with their favorite sports. Our comprehensive schedules for today cover a wide array of sports, including football, basketball, baseball, hockey, motorsports, soccer, and more. Tonight features a men's Big Ten basketball match up between Purdue vs Illinois streaming on Peacock. The Boilermakers have taken the last four wins against the Fighting Illini including their last game on January 5. Both teams enter tonight on a 3-game win streak. Pre-game coverage begins at 6:30 pm ET. 10:00 AM EST. 2024 NFL Scouting Combine. 01:00 PM EST. 2024 NFL Scouting Combine. 09:00 PM EST. NFL Total Access. View the full NFL Network Schedule! Listings for all NFL Network programs -Good ... The NFL playoffs are here and Wild Card Weekend is well underway, with four of six games wrapped up over the weekend. On Saturday, action kicked off with C.J. Stroud and the Texans defeating Joe Flacco and the Browns in dominant fashion, 45-14.In sub-zero temperatures with Taylor Swift watching on, Patrick Mahomes and the Chiefs took down Tua Tagovailoa and the Dolphins 26-7.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': \"I'd like to figure out what games are on tonight.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
