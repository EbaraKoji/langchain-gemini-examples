{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain with Youtube Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "llm = VertexAI(model_name=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Transcript with YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.youtube import YoutubeLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url('https://youtu.be/pNcQ5XXMgH4', add_video_info=True)\n",
    "result = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"what is going on good people again right now we have a super exciting tutorial because we are going to take YouTube transcripts and we're going to pass them to open Ai and the way that we're going to do that is via a library called Lang chain which is what this entire series is about now before we jumped into it I wanted to show a diagram again I think these diagrams are helpful but you have to let me know so just let me know in the comments here so I wanted to do an overview about what we're actually going to be writing out in code because I think it's a little easier to see in pictures first so the way this is going to work is we're going to have a video a YouTube video we're going to pass it we're going to pass it a URL and then what Lang chain is going to help us do is it's going to help us load this video as a document and a document just means you're going to be taking the transcript which is the text of the video and you're going to be loading it as a document which is something that lane chain can help understand now with that document we're then going to go generate a summary of it and the way that link chain is going to do this is it's going to create a prompt for us that says hey generate me a concise summary of the following text and then it's going insert the transcript of the YouTube video which is pretty sweet and this is going to happen in open Ai and this is going to happen to be an API call and then what we get out the other end is open AI is going to tell us hey this video is about XYZ now an interesting part about this and where it gets kind of confusing is well what happens if your video is too long oh no our video two is too long we can't pass this because say you're looking at a YouTube video and it's like an hour long well you can't pass all that transcript into open AI because they have a token limit and this is where a lot of the ergonomics of Lang chain really come to help out here now what we're going to do is we're actually going to split up that text so we're going to still see that it's from video two but we're going to have our document one document two document three and then what Lang chain is going to help us do is it's going to go to open Ai and it's going to say hey I want you to generate a summary for me of document one generate of document 2 generate of document three now the cool part about this is that this is all under the hood the cool part is then what it's going to do is it's going to say hey please generate me a summary of these summaries and then all of a sudden open AI is going to give us a summary of the summaries and the conclusion you get with the video is all the way about now this is one method of kind of combining documents like this and this is called the map reduce method but we'll get into that in a second when we talk about the different chain types all right that's enough diagrams let's look at some code here all right now that we're looking at some code here our first import statements uh this the star of the show here is going to be the YouTube loader this is going to be the tool that is going to help us do this we're going to uh import open Ai and we're going to import load summarize chain because this is going to be the chain that's going to help summarize for us so let's go ahead and run those I also had to install YouTube's transcripts API and then also pytube as well in case you uh run on that same problem so with the YouTube loader we're going to call Dot from YouTube URL and we are going to pass it a single YouTube url here and what that'll do is we're going to store that in a loader so to get it ready and kind of stage it and then we're actually going to call Dot load on it which is going to do the loading for us and I wanted to print this out and show you what we have here so if we have if we look at this result you can see that the result is a list of items it's very important we'll talk about this in a second year and then we just have some metadata on it but it is going to be a list of documents and these are the things that lane chain can help understand and can process for us and in this document you can see here that there's a page context which is going to be the transcript that is from this video and then we also have some interesting metadata too about the video itself but I'm going to go ahead and close this here we're going to uh instantially oh I want I need to load the open AI key we're going to initialize our large language model which is going to be the open AI one and then we're going to call load summarize chain we're going to pass it our model we're going to say chain type equals stuff important here we're going to talk about why this is changing later we're going to say verbose equals false because we don't want to see anything and then we're going to pass it the result that we loaded in which is the the document or the list of documents that we had let's go ahead and run this and then all of a sudden we get cool Pedro Pascal shared his experiences shooting HBO's Last of Us awesome so just based off the transcript it has a some summary of the YouTube video for us nice but what if you have a long video so I wanted to show you this one here we have another YouTube video which is going to be a podcast of my first million on here we hear some Sean talk and you can see that it is going to be almost 60 Minutes long and this is quite long and spoiler alert it's too long for open AI uh for the token limit that they have so let me show you this though we're going to load this in we're going to load the result you can see it takes a little bit and then we're going to say load summarize chain okay cool with chain type equals stuff and we're going to run this result here and then oh no we have an error it's trying to do something up here and it says this model's maximum context is uh 4097 tokens you've requested almost fifteen thousand and that's no good because that's too long so in the old days before Lang chain what we'd have to do here is we'd have to figure out some way to either run multiple pieces ourselves manually copy and paste it'd be a freaking mess we don't want to do any of that stuff so the problem is your transcript or your document is too long now what we're going to do here is we're actually going to split up that document which is what we saw earlier on the diagram and so I'm going to load in the recursive character splitter and I'm going to get this loaded here and I'm just going to set a chunk size of 2000. you can play with this it might be different for your use case whatever you want but if you're not getting what you need try switching this variable if you want some help there I'm going to load up that text footer and now what I'm going to do is I'm going to load in that single YouTube video into the text splitter and what it's going to do for me is actually I want to show you this here uh text and so let's let's first check out the type of text it is going to be a list okay cool let's see what it's a list of and you can see here it's a list of documents and this page context is still quite long but it's we're aiming for a chunk size of about 2 000. I just want to show you what a chunk size of 100 would look like and so we have a a list of documents again with a page context and this page context is only about a hundred characters long ish or 100 tokens long-ish it's it's uh it's interesting there and so if we were to look at no I don't want to do type I want to do length so if we're to do length of how many texts we have we have 522 and that's because it's taking our entire transcript and it's basically putting it into chunks roughly of a of a hundred if we're to do a thousand for chunks you can see here it's roughly 10 times less which is going to be on the 51. so this is a way to split up your documents and so now we have a whole bunch of documents um that are length of what we set right here but I'm going to set this back to 2000. nice and then what we're going to do is I'm going to call the llm here but I'm going to change the chain type and in fact before we did this I want to I want to show you the issue here um let's do chunk size 2000 and then we're going to do stuff and I'm going to call run and let's do oh I want to do this on text let's do run right here and so the issue is that we have again this is the maximum model length but we've requested all these documents together because when you do chain type equals stuff what you're doing is you're saying the Lang chain hey I want you to take all my documents and stuff them into the prompt that you're feeding open AI now there's a way around this not a way around this but an alternative is if you change the type to mapreduce that is when you're going to start to say hey just give me a summary of all these different documents that you have and then generate me a final summary so if we change it to mapreduce I'm going to go ahead and run this and let's give this a sec because this is going to make multiple API calls because what it's actually doing is it's making a uh it's telling hey open AI I want you to give me a summary of each one of these different documents and you saw how we had quite a uh a few number of documents cool well nice so we just had this long transcript and what we just had is now we have the summary of what this transcript says but I wanted to show you what it this actually looks like underneath the covers of what um Lang chain is doing and so what I'm going to do here is I'm going to set for both equals true which gives you insight as to the calls that laying chain is making the open AI this is going to get kind of confusing so I just want to do the first four documents on here which is you know the first little bit of the video that we loaded and so what we're going to look at here is we're going to look at all right we're doing a mapreduce document chain cool and so the very first call that it's saying to open AI is write me a concise summary of The Following nice so here is the following statement and this is one of the document chunks that we submitted beforehand and then it's saying hey again I want you to write me a concise summary of the following now here's the second document that we wanted it to summarize and then here's the third document and then here's the fourth document now the cool part is what you can see that gets returned is we have four different summaries of four different documents so summary One summary two summary three and summary four and the reason why is because we just wanted to see the first four that we had up here so we have all those summaries and then what it said was is basically write me a concise summary of the following so a summary of the summaries and then what we get is we get this uh summary of the summaries that's right here nice um it's cool now what if you have multiple videos that you want to do well in this case I have a YouTube url list I'm just passing it two different videos I'm going to get a list ready that is going to hold my text for me I'm going to get my uh character splitter ready and I'm going to say hey for URL in this list of URLs I want you to load up the video or get the loader ready I want you to load the video and then I want you to extend this list with the documents that you've split it into so in this case I have two YouTube videos I'm just going to go through both of them right there and then I'm going to call the summarize scan again with mapreduce in this case I don't really want to do verbose equals true because you already saw what that looked like but now what it's doing is it's going through both those videos it's split it splitting them up into separate documents in case that they're in case they're too long and then it's generating a summary for me now these were two videos about two completely different things and so this it starts off with a golf video about how to build a golf course in your backyard so it says cool blah blah looks great and then now it goes into the second summary which is around uh a uh interview between Bella Ramsay and Pedro Pascal about what they were doing so that is how you do uh loading up YouTube videos with a transcript and with the summaries I hope that that was helpful for you please let me know if the diagram was helpful I'm happy to do more videos and as always please leave please leave comments about how we can improve the videos and about your own personal uh business problems that we can help solve I'll see you later\", metadata={'source': 'pNcQ5XXMgH4', 'title': 'LangChain 101: YouTube Transcripts + OpenAI', 'description': 'Unknown', 'view_count': 21253, 'thumbnail_url': 'https://i.ytimg.com/vi/pNcQ5XXMgH4/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCmP9TXvB4nm22ZX7b5Tl0AagEU3A', 'publish_date': '2023-02-23 00:00:00', 'length': 668, 'author': 'Greg Kamradt (Data Indy)'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['page_content', 'metadata', 'type'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pNcQ5XXMgH4',\n",
       " 'title': 'LangChain 101: YouTube Transcripts + OpenAI',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 21253,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/pNcQ5XXMgH4/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCmP9TXvB4nm22ZX7b5Tl0AagEU3A',\n",
       " 'publish_date': '2023-02-23 00:00:00',\n",
       " 'length': 668,\n",
       " 'author': 'Greg Kamradt (Data Indy)'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is going on good people again right now we have a super exciting tutorial because we are going '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type='stuff', verbose=False)\n",
    "\n",
    "# The result is long! Though gemini can handle long inputs, it should be better to split the text.\n",
    "# chain.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type='map_reduce', verbose=False)\n",
    "output = chain.invoke(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Langchain can extract transcripts from YouTube videos and pass them to OpenAI for summarization',\n",
       " \"To handle long transcripts, split them into smaller chunks using the Recursive Character Splitter and use Langchain's map-reduce method to generate summaries\",\n",
       " \"OpenAI's Summarize function summarizes individual documents, and the collective summaries provide an overview of the entire document set\",\n",
       " 'The code demonstrates how to load, split, and summarize YouTube videos using Langchain, YouTube Loader, and SummarizeChain libraries.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['output_text'].split('. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using load_qa_chain with retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embeddings = VertexAIEmbeddings('textembedding-gecko@001')\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = '''Answer the question based on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = ({\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangChain is a tool that helps users understand and process documents. It can generate summaries of documents, including YouTube transcripts, using OpenAI's large language model. LangChain can also split up long documents into smaller chunks and generate summaries of each chunk. Additionally, it can combine summaries of multiple documents into a single summary. This tool can be helpful for quickly getting an overview of the content of a document or for understanding the main points of a video.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'What can I do with langchain? Please explain in a brief paragraph.'\n",
    "chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating outputs with other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "template = '''Answer the question based on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "'''\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = ({\n",
    "    'context': itemgetter('question') | retriever,\n",
    "    'question': itemgetter('question'),\n",
    "    'language': itemgetter('language'),\n",
    "}\n",
    "         | prompt\n",
    "         | llm\n",
    "         | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchainを使用すると、YouTubeの文字起こしをOpenAIに渡すことができます。'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question': 'What can I do with langchain, according to the video?', 'language': 'japanese'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YouTube動画の文字起こしを使って、OpenAIで動画の概要を生成する方法'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question': 'What can I learn from the video? Please explain in a brief sentence.', 'language': 'japanese'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' このビデオで提供されているようなサービスを開発するための前提条件として、私は次のように考えています。\\n\\n* 適切なプログラミング言語とフレームワークの知識\\n* 自然言語処理の深い理解\\n* 機械学習の基礎的な知識\\n* クラウドコンピューティングプラットフォームに関する経験\\n* 堅牢でスケーラブルなシステムを設計する能力\\n* ユーザーエクスペリエンスの設計に関する知識\\n* 関連する規制の遵守と倫理的配慮に関する認識'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    'question': 'What do you think the prerequisites of developing the services like the ones offered in this video?',\n",
    "    'language': 'japanese'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Transcript of other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url('https://youtu.be/gy0jCRapP34', add_video_info=True, language=['ja'])\n",
    "result = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''Write a summary of the following text in Japanese:\n",
    "\n",
    "'{text}'\n",
    "\n",
    "CONCISE SUMMARY:'''\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=['text'])\n",
    "chain = load_summarize_chain(\n",
    "    llm,  # e.g. ChatOpenAI(temperature=0)\n",
    "    chain_type=\"stuff\",\n",
    "    verbose=False,\n",
    "    prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({'input_documents': texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['この動画は、カニクリームコロッケにカニが必要かどうか検証する内容です',\n",
       " '出演者たちが目隠しをしてカニクリームコロッケと普通のクリームコロッケを食べ、どちらかを当てました',\n",
       " '結果は、3人がカニクリームコロッケを、2人がクリームコロッケを特定できました',\n",
       " 'カニクリームコロッケはカニの風味が強いため、特定しやすいことがわかりました',\n",
       " '']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['output_text'].split('。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "embeddings = VertexAIEmbeddings('textembedding-gecko@001')\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = '''Answer the question based on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "'''\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = ({\n",
    "    'context': itemgetter('question') | retriever,\n",
    "    'question': itemgetter('question'),\n",
    "    'language': itemgetter('language'),\n",
    "}\n",
    "         | prompt\n",
    "         | llm\n",
    "         | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({'question': 'What is the humor of this video?', 'language': 'japanese'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['カニクリームコロッケにカニが入っていても、入っていなくても、味が変わらないことを検証するという企画に対して、池崎が「カニは不要」と主張']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(output).split('。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['カニクリームコロッケのカニの有無を検証するという、一見するとくだらない企画を、サンシャイン池崎の独特なトークやメンバーのリアクションで笑いへと昇華させている点',\n",
       " '']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chain.invoke({'question': 'What is the humor of this video?', 'language': 'japanese'})\n",
    "str(output).split('。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['かにクリームコロッケにかに入っていても入っていなくてもわからないかもしれない、という仮説を検証する企画',\n",
       " '検証結果、かにが入っていないかにクリームコロッケとかにが入っているかにクリームコロッケを区別することができなかった',\n",
       " 'この結果から、かにクリームコロッケにはかにを入れる必要がないのではないか、という結論に至る',\n",
       " '']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chain.invoke({'question': 'What is the humor of this video?', 'language': 'japanese'})\n",
    "str(output).split('。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答に再現性がない。おそらくLLMの性能以外の問題。\\\n",
    "Transcriptが不正確だったり、複数人の会話で誰の発言か分からなかったりすると、適切に解釈をすることができない。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
