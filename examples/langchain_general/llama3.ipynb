{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama with LangChain and Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model='llama3', temperature=0, num_predict=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Please explain LLM in detail.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great topic!\n",
      "\n",
      "LLM stands for Large Language Model, which is a type of artificial intelligence (AI) model designed to process and generate human-like language. In this explanation, I'll cover the basics, architecture, training, and applications of LLMs.\n",
      "\n",
      "**What is an LLM?**\n",
      "\n",
      "An LLM is a neural network-based AI model that can understand and generate natural language text. It's trained on vast amounts of text data to learn patterns, relationships, and context within language. This allows LLMs to perform various tasks, such as:\n",
      "\n",
      "1. **Language understanding**: LLMs can comprehend the meaning of text, including nuances like sarcasm, idioms, and figurative language.\n",
      "2. **Text generation**: They can generate new text based on input prompts or patterns learned from training data.\n",
      "3. **Question answering**: LLMs can answer questions by retrieving relevant information from their training data.\n",
      "\n",
      "**Architecture**\n",
      "\n",
      "LLMs typically consist of three main components:\n",
      "\n",
      "1. **Encoder**: This component takes in a sequence of tokens (e.g., words, characters) and converts them into a continuous representation, called the \"contextualized embedding.\" The encoder is usually based on transformer architectures, which are particularly well-suited for sequential data like text.\n",
      "2. **Decoder**: The decoder takes the contextualized embedding as input and generates a sequence of tokens (output) based on the input prompt or pattern learned from training data.\n",
      "3. **Attention mechanism**: This component allows the model to focus on specific parts of the input sequence when generating output. Attention helps the model attend to relevant information in the input text, which is essential for tasks like language understanding and generation.\n",
      "\n",
      "**Training**\n",
      "\n",
      "LLMs are trained using a combination of supervised and unsupervised learning techniques:\n",
      "\n",
      "1. **Supervised learning**: The model is trained on labeled datasets, where the target output is provided along with the input text. This helps the model learn to generate specific types of text (e.g., summaries, translations).\n",
      "2. **Unsupervised learning**: The model is trained on large amounts of unlabeled text data, which allows it to learn general language patterns and relationships.\n",
      "\n",
      "The training process typically involves:\n",
      "\n",
      "1. **Pre-training**: The model is pre-trained on a large corpus of text data using masked language modeling (MLM) or other self-supervised learning objectives.\n",
      "2. **Fine-tuning**: The pre-trained model is fine-tuned on specific tasks, such as question answering or text generation, using labeled datasets.\n",
      "\n",
      "**Applications**\n",
      "\n",
      "LLMs have numerous applications in various domains:\n",
      "\n",
      "1. **Natural Language Processing (NLP)**: LLMs can be used for NLP tasks like language translation, sentiment analysis, and text summarization.\n",
      "2. **Chatbots and virtual assistants**: LLMs can power conversational AI systems that understand and respond to user input.\n",
      "3. **Content generation**: LLMs can generate content, such as articles, stories, or even entire books, based on input prompts or patterns learned from training data.\n",
      "4. **Question answering**: LLMs can be used for question answering applications, like providing answers to customer support queries or generating responses to user questions.\n",
      "\n",
      "**Challenges and Limitations**\n",
      "\n",
      "While LLMs have made significant progress in recent years, they still face challenges and limitations:\n",
      "\n",
      "1. **Lack of common sense**: LLMs may not always understand the nuances of human language, leading to incorrect or nonsensical outputs.\n",
      "2. **Bias and fairness**: LLMs can perpetuate biases present in their training data, which is a significant concern for applications like content generation or question answering.\n",
      "3. **Scalability**: Training large-scale LLMs requires significant computational resources and energy consumption.\n",
      "\n",
      "In conclusion, Large Language Models are powerful AI models that have the potential to revolutionize various domains by enabling human-like language understanding and generation. While they face challenges and limitations, ongoing research and advancements in areas like transformer architectures, attention mechanisms, and training techniques will continue to improve their capabilities and applications.\n"
     ]
    }
   ],
   "source": [
    "query = 'Please explain LLM in detail.'\n",
    "\n",
    "result = llm.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great topic!\n",
      "\n",
      "LLM stands for Large Language Model, which is a type of artificial intelligence (AI) model designed to process and generate human-like language. In this explanation, I'll cover the basics, architecture, training, and applications of LLMs.\n",
      "\n",
      "**What is an LLM?**\n",
      "\n",
      "An LLM is a neural network-based AI model that can understand and generate natural language text. It's trained on vast amounts of text data to learn patterns, relationships, and context within language. This allows LLMs to perform various tasks, such as:\n",
      "\n",
      "1. **Language understanding**: LLMs can comprehend the meaning of text, including nuances like sarcasm, idioms, and figurative language.\n",
      "2. **Text generation**: They can generate new text based on input prompts or patterns learned from training data.\n",
      "3. **Question answering**: LLMs can answer questions by retrieving relevant information from their training data.\n",
      "\n",
      "**Architecture**\n",
      "\n",
      "LLMs typically consist of three main components:\n",
      "\n",
      "1. **Encoder**: This component takes in a sequence of tokens (e.g., words, characters) and converts them into a continuous representation, called the \"contextualized embedding.\" The encoder is usually based on transformer architectures, which are particularly well-suited for sequential data like text.\n",
      "2. **Decoder**: The decoder takes the contextualized embedding as input and generates a sequence of tokens (output) based on the input prompt or pattern learned from training data.\n",
      "3. **Attention mechanism**: This component allows the model to focus on specific parts of the input sequence when generating output. Attention helps the model attend to relevant information in the input text, which is essential for tasks like language understanding and generation.\n",
      "\n",
      "**Training**\n",
      "\n",
      "LLMs are trained using a combination of supervised and unsupervised learning techniques:\n",
      "\n",
      "1. **Supervised learning**: The model is trained on labeled datasets, where the target output is provided along with the input text. This helps the model learn to generate specific types of text (e.g., summaries, translations).\n",
      "2. **Unsupervised learning**: The model is trained on large amounts of unlabeled text data, which allows it to learn general language patterns and relationships.\n",
      "\n",
      "The training process typically involves:\n",
      "\n",
      "1. **Pre-training**: The model is pre-trained on a large corpus of text data using masked language modeling (MLM) or other self-supervised learning objectives.\n",
      "2. **Fine-tuning**: The pre-trained model is fine-tuned on specific tasks, such as question answering or text generation, using labeled datasets.\n",
      "\n",
      "**Applications**\n",
      "\n",
      "LLMs have numerous applications in various domains:\n",
      "\n",
      "1. **Natural Language Processing (NLP)**: LLMs can be used for NLP tasks like language translation, sentiment analysis, and text summarization.\n",
      "2. **Chatbots and virtual assistants**: LLMs can power conversational AI systems that understand and respond to user input.\n",
      "3. **Content generation**: LLMs can generate content, such as articles, stories, or even entire books, based on input prompts or patterns learned from training data.\n",
      "4. **Question answering**: LLMs can be used for question answering applications, like providing answers to customer support queries or generating responses to user questions.\n",
      "\n",
      "**Challenges and Limitations**\n",
      "\n",
      "While LLMs have made significant progress in recent years, they still face challenges and limitations:\n",
      "\n",
      "1. **Lack of common sense**: LLMs may not always understand the nuances of human language, leading to incorrect or nonsensical outputs.\n",
      "2. **Bias and fairness**: LLMs can perpetuate biases present in their training data, which is a significant concern for applications like content generation or question answering.\n",
      "3. **Scalability**: Training large-scale LLMs requires significant computational resources and energy consumption.\n",
      "\n",
      "In conclusion, Large Language Models are powerful AI models that have the potential to revolutionize various domains by enabling human-like language understanding and generation. While they face challenges and limitations, ongoing research and advancements in areas like transformer architectures, attention mechanisms, and training techniques will continue to improve their capabilities and applications."
     ]
    }
   ],
   "source": [
    "for chunks in llm.stream(query):\n",
    "    print(chunks, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
