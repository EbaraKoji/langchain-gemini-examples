{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "llm = VertexAI(model_name='gemini-pro', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.embeddings.voyageai import VoyageEmbeddings\n",
    "from langchain.llms.huggingface_hub import HuggingFaceHub\n",
    "from langchain.embeddings.huggingface_hub import HuggingFaceHubEmbeddings\n",
    "\n",
    "\n",
    "embedding = VertexAIEmbeddings('textembedding-gecko@001')\n",
    "\n",
    "# XXX: æ€§èƒ½ãŒgeckoã¨æ¯”è¼ƒã—ã¦è‘—ã—ãæ‚ªã„!!!\n",
    "# embedding = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=os.environ['GOOGLE_GENERATIVE_LANGUAGE_API_KEY'])\n",
    "\n",
    "# XXX: ERROR!!!embed_contentãƒ¡ã‚½ãƒƒãƒ‰ãŒå®Ÿè£…ã•ã‚Œã¦ã„ãªã„ã®ã§retrieverã®embeddingã¨ã—ã¦ä½¿ãˆãªã„!!\n",
    "# embedding = GoogleGenerativeAIEmbeddings(model='models/embedding-gecko-001', google_api_key=os.environ['GOOGLE_GENERATIVE_LANGUAGE_API_KEY'])\n",
    "\n",
    "# XXX: responseãŒã¨ã¦ã‚‚é…ã„!æ€§èƒ½ã‚‚æ‚ªã„ï¼!\n",
    "# embedding = HuggingFaceHubEmbeddings(repo_id='intfloat/multilingual-e5-base')\n",
    "\n",
    "# TODO: API KEYã®æœ‰åŠ¹åŒ–(èª²é‡‘)ã€æ€§èƒ½æ¯”è¼ƒ\n",
    "# embedding = VoyageEmbeddings(voyage_api_key=os.environ['CLAUDE_API_SECRET_KEY'])Ã˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.web_base import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(web_path='https://python.langchain.com/docs/modules/memory/')\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "retriever = Chroma.from_documents(documents=splits, embedding=embedding) \\\n",
    "    .as_retriever(search_kwargs={'k': min(len(docs), 4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever('How to add memory to a chain? Please explain with an code example.')\n",
    "result = retriever.get_relevant_documents('How to add memory to a chain? Please explain with an code example.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain provides a lot of utilities for adding memory to a system.\n",
      "These utilities can be used by themselves or incorporated seamlessly into a chain.Most of memory-related functionality in LangChain is marked as beta. This is for two reasons:Most functionality (with some exceptions, see below) are not production readyMost functionality (with some exceptions, see below) work with Legacy chains, not the newer LCEL syntax.The main exception to this is the ChatMessageHistory functionality. This functionality is largely production ready and does integrate with LCEL.LCEL Runnables: For an overview of how to use ChatMessageHistory with LCEL runnables, see these docsIntegrations: For an introduction to the various ChatMessageHistory integrations, see these docsIntroductionâ€‹A memory system needs to support two basic actions: reading and writing.\n",
      "Recall that every chain defines some core execution logic that expects certain inputs.\n"
     ]
    }
   ],
   "source": [
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation.',\n",
       " 'language': 'en',\n",
       " 'source': 'https://python.langchain.com/docs/modules/memory/',\n",
       " 'title': '[Beta] Memory | ğŸ¦œï¸ğŸ”— Langchain'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata['source'] == result[0].metadata['source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ€§èƒ½ã¯Claudeä»¥å¤–ã§ã¯åœ§å€’çš„ã«geckoãƒ¢ãƒ‡ãƒ«ãŒå¼·ã„ã€‚\n",
    "\n",
    "ãŸã ã—ã€geckoãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚‚ã€example codeã¯å–å¾—ã§ãã¦ã„ãªã„ã€‚\n",
    "\n",
    "çµå±€ã€geminiã‚’ä½¿ã†å ´åˆã¯ã‚ã‚‹ç¨‹åº¦é•·ã„contextã§ã‚‚å•é¡Œãªãå…¥ã‚Œã‚‰ã‚Œã‚‹ã®ã§ã€ã€ŒåŸºæœ¬ã¯Webã‚µã‚¤ãƒˆå…¨ä½“ã®page contentã‚’contextã«ãã®ã¾ã¾ä»£å…¥ã€ã€Œè¤‡æ•°ã®Webã‚µã‚¤ãƒˆã‚’RAGã«ä½¿ã†å ´åˆã®ã¿retrieverã‚’ä½¿ç”¨ã—ã¦metadataã‹ã‚‰è©²å½“ã™ã‚‹Webã‚µã‚¤ãƒˆã‚’ç‰¹å®šã—ã¦ã‚„ã¯ã‚Šã‚µã‚¤ãƒˆå…¨ä½“ã®page contentã‚’ä»£å…¥ã€ã®æ–¹ãŒç²¾åº¦ãŒã„ã„å¯èƒ½æ€§ãŒé«˜ã„ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
